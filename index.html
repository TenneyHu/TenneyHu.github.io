<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Tianyi (Tenney) Hu </title>
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
		<link href="style.css" rel="stylesheet">
	</head>

	<body>
		<h1 style="text-align: center">Tianyi (Tenney) Hu</h1>
		<div class="container">
			<div style="flex-basis: 74%">
				<p>
				I'm a fifth-year PhD student in the
				<a href="https://nlp.cs.princeton.edu/">Princeton NLP group</a>,
				working with
				<a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>.
				For the summer and fall of 2023, I was a student researcher at Google Research, where I was advised by
				<a href="https://web.media.mit.edu/~asma_gh/">Asma Ghandeharioun</a> and
				<a href="https://lampinen.github.io/">Andrew Lampinen</a>.
				Before this I was a software engineer at Google and IBM,
				and before that I was an undergraduate at Yale where I worked
				with <a href="https://bobfrank1.github.io/">Bob Frank</a> and
				<a href="http://www.cs.yale.edu/homes/radev/">Dragomir Radev</a>,
				and received a BA in English.
				</p>
				<p style="text-align:center">
				<a href="mailto:dfriedman@princeton.edu">Email</a> &nbsp;/&nbsp;
				<a href="https://scholar.google.com/citations?user=1UMQ_KwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
				<a href="https://twitter.com/danfriedman0">Twitter</a> &nbsp;/&nbsp;
				<a href="https://github.com/danfriedman0/">Github</a>
				</p>
			</div>
			<img src="profile.jpg" width="200" height="246" style="flex-basis: 24%">
		</div>
		<h2>Research Interests</h2>
		<p>
		I'm currently interested in making large, neural language models easier to understand.
		One direction I'm especially interested in is to design models that are
		<a href="https://arxiv.org/abs/2306.01128">inherently interpretable</a>,
		so that we can automatically convert models into formats that are easier to inspect and understand, such as discrete computer programs.
		I'm also interested in approaches that take a more behavioral view, to better characterize the strengths and limitations of large language models
		(<a href="https://arxiv.org/abs/2309.13638">example 1</a>;
		<a href="https://arxiv.org/abs/2305.13299">example 2</a>).
		Some of my more general interests include unsupervised structure learning, formal languages,
		probabilistic models, and inductive bias.
		I'm also interested in applications of NLP to humanities research
		and am involved with the
		<a href="https://cdh.princeton.edu/">
			Princeton Center for Digital Humanities
		</a>.
		</p>
		<h2>Publications and preprints</h2>
		<ul>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2407.10949">
					Representing Rule-based Chatbots with Transformers
				</a>
				<br>
				<strong>Dan Friedman</strong>, Abhishek Panigrahi, Danqi Chen
				<br>
				arXiv 2024
				<br>
				<a href="https://arxiv.org/abs/2407.10949">Paper</a> | <a href="https://github.com/princeton-nlp/ELIZA-Transformer">Code</a>
			</li>
			<br>
		</ul>
	</body>

</html>
