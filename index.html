<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Tianyi (Tenney) Hu </title>
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
		<link href="style.css" rel="stylesheet">
	</head>

	<body>
		<h1 style="text-align: center">Tianyi (Tenney) Hu</h1>
		<div class="container">
			<div style="flex-basis: 74%">
				<p>
				I am a second-year MsC student in the Department of Computer Science at University of Copenhagen, 
				advised by <a href="https://danielhers.github.io/">Daniel Hershcovich</a> 
				and <a href="https://di.ku.dk/english/staff/?pure=en/persons/641366">Maria Maistro</a>.	
				I was a Researcher and Engineer in Basic Search Team at Baidu Search during 2018-2022, 
				where I worked on improving the timeliness of the search engine.
				I led a small machine learning team working on the 'Hot Discussion' project, 
				aimed at enhancing the timeliness and content diversity in search engines by providing social media results like Weibo.
				and before that I was an undergraduate at Dalian University of Technology and recieved my BSc in Software Engineering in 2019.
				</p>
				<p style="text-align:center">
				<a href="mailto:tenneyhu@gmail.com">Email</a> &nbsp;/&nbsp;
				<a href="https://scholar.google.com/citations?user=1UMQ_KwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
				<a href="https://github.com/TenneyHu/">Github</a>
				</p>
			</div>
			<img src="profile.jpg" width="200" height="246" style="flex-basis: 24%">
		</div>
		<h2>Research Interests</h2>
		<p>
		I'm currently interested in making large, neural language models easier to understand.
		One direction I'm especially interested in is to design models that are
		<a href="https://arxiv.org/abs/2306.01128">inherently interpretable</a>,
		so that we can automatically convert models into formats that are easier to inspect and understand, such as discrete computer programs.
		I'm also interested in approaches that take a more behavioral view, to better characterize the strengths and limitations of large language models
		(<a href="https://arxiv.org/abs/2309.13638">example 1</a>;
		<a href="https://arxiv.org/abs/2305.13299">example 2</a>).
		Some of my more general interests include unsupervised structure learning, formal languages,
		probabilistic models, and inductive bias.
		I'm also interested in applications of NLP to humanities research
		and am involved with the
		<a href="https://cdh.princeton.edu/">
			Princeton Center for Digital Humanities
		</a>.
		</p>
		<h2>Publications</h2>
		<ul>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2407.10949">
					Representing Rule-based Chatbots with Transformers
				</a>
				<br>
				<strong>Dan Friedman</strong>, Abhishek Panigrahi, Danqi Chen
				<br>
				arXiv 2024
				<br>
				<a href="https://arxiv.org/abs/2407.10949">Paper</a> | <a href="https://github.com/princeton-nlp/ELIZA-Transformer">Code</a>
			</li>
			<br>
		</ul>
	</body>

</html>
